{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution: MF of an explicit feedback (ratings) matrix\n",
    "\n",
    "**Given:**\n",
    "- data loader and matrix initialization code (step1+2)\n",
    "- default params settings, training and evaluation (steps 5 to 8)\n",
    "\n",
    "**Answer the following questions:**\n",
    "- Q1: Implement L2 and L2 losses and evaluate with default params (step 3)\n",
    "- Q2: Implement L1 and L2 regularizations with default params (step 4)\n",
    "- Q3: Bench the parameters around the default values\n",
    "\n",
    "**Original source:** http://hameddaily.blogspot.fr/2016/12/simple-matrix-factorization-with.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n",
      "Mean (train) rating = 3.529004007930282\n",
      "Number of ratings (train/val/total) = 70111/29889/100000\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "print(tf.__version__)\n",
    "# read data\n",
    "df = pd.read_csv('data/ml-100k/u.data', sep='\\t', names=['user', 'item', 'rate', 'time'])\n",
    "numpy.random.seed(42)\n",
    "msk = numpy.random.rand(len(df)) < 0.7\n",
    "df_train = df[msk]\n",
    "df_test = df[~msk]\n",
    "\n",
    "user_index = [x-1 for x in df_train.user.values]\n",
    "item_index = [x-1 for x in df_train.item.values]\n",
    "user_index_test = [x-1 for x in df_test.user.values]\n",
    "item_index_test = [x-1 for x in df_test.item.values]\n",
    "\n",
    "rates = df_train.rate.values\n",
    "rates_test = df_test.rate.values\n",
    "num_ratings = len(rates)\n",
    "num_ratings_test = len(rates_test)\n",
    "mean_rating = numpy.mean (rates)\n",
    "\n",
    "print (\"Mean (train) rating = \" + str(mean_rating))\n",
    "print (\"Number of ratings (train/val/total) = \" + str(num_ratings) + \"/\" + str(num_ratings_test) + \"/\" + str(num_ratings + num_ratings_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define/initialize the User and Item matrices and use their product to compute initial ratings R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "feature_len = 10\n",
    "num_users = 943\n",
    "num_items = 1682\n",
    "\n",
    "U = tf.Variable(initial_value=tf.truncated_normal([num_users,feature_len]), name='users')\n",
    "P = tf.Variable(initial_value=tf.truncated_normal([feature_len,num_items]), name='items')\n",
    "result = tf.matmul(U, P)\n",
    "result_flatten = tf.reshape(result, [-1])\n",
    "\n",
    "# rating\n",
    "result_values = tf.gather(result_flatten, user_index * tf.shape(result)[1] + item_index, name='predicted_ratings')\n",
    "result_values_test = tf.gather(result_flatten, user_index_test * tf.shape(result)[1] + item_index_test, name='validation_ratings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the cost function (try L2, L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between the predicted ratings and the actual\n",
    "# ratings. The predicted ratings are the values obtained form the matrix\n",
    "# multiplication with the mean rating added on.\n",
    "\n",
    "# L1 cost function\n",
    "#diff_op = tf.subtract(tf.add(result_values, mean_rating, name=\"add_mean\"), rates, name='trainig_diff')\n",
    "#diff_op_abs = tf.square(diff_op, name=\"abs_difference\")\n",
    "#base_cost = tf.reduce_sum(diff_op_abs, name=\"sum_abs_error\")\n",
    "\n",
    "# L2 cost function\n",
    "diff_op = tf.subtract(tf.add(result_values, mean_rating, name=\"add_mean\"), rates, name='trainig_diff')\n",
    "diff_op_squared = tf.square(diff_op, name=\"squared_difference\")\n",
    "base_cost = tf.reduce_sum(diff_op_squared, name=\"sum_squared_error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Add regularization term for User, Item matrices (try L2, L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 regularization\n",
    "l1_norm_sums = tf.add(tf.reduce_sum(tf.abs(U, name='user_abs'), name='user_norm'),\n",
    "                   tf.reduce_sum(tf.abs(P, name='item_abs'), name='item_norm'))\n",
    "\n",
    "# L2 regularization\n",
    "l2_norm_sums = tf.add(tf.reduce_sum(tf.square(U, name='user_abs'), name='user_norm'),\n",
    "                   tf.reduce_sum(tf.square(P, name='item_abs'), name='item_norm'))\n",
    "\n",
    "# regularized cost\n",
    "with tf.name_scope(\"training_cost\") as scope:\n",
    "    lamda = tf.constant(.001, name='lambda')\n",
    "    regularizer = tf.multiply(l2_norm_sums, lamda, 'regularizer')\n",
    "    #regularizer = tf.mul(l1_norm_sums, lamda, 'regularizer')\n",
    "    regularized_cost = tf.add(base_cost, regularizer)\n",
    "    tf.summary.scalar('reg_cost', regularized_cost)\n",
    "    \n",
    "# test cost\n",
    "diff_op_test = tf.subtract(tf.add(result_values_test, mean_rating, name=\"add_mean\"), rates_test, name='trainig_diff_test')\n",
    "with tf.name_scope(\"validation_cost\") as scope:\n",
    "    cost_test = tf.div(tf.reduce_sum(tf.square(diff_op_test, name=\"squared_difference_test\"), name=\"sum_squared_error_test\"), num_ratings_test * 2, name=\"average_error\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Setup training (learning rate, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cost function\n",
    "\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    lr = tf.constant(.001, name='learning_rate')\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(lr, global_step, 10000, 0.96, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "    training_step = optimizer.minimize(regularized_cost, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define accuracy\n",
    "threshold = 1.0\n",
    "with tf.name_scope(\"training_accuracy\") as scope:\n",
    "  # Just measure the absolute difference against the threshold\n",
    "  good = tf.less(tf.abs(diff_op), threshold)\n",
    "\n",
    "  accuracy_tr = tf.div(tf.reduce_sum(tf.cast(good, tf.float32)), num_ratings)\n",
    "  \n",
    "with tf.name_scope(\"validation_accuracy\") as scope:\n",
    "  # Validation set accuracy:\n",
    "  good_test = tf.less(tf.abs(diff_op_test), threshold)\n",
    "  accuracy_test = tf.reduce_sum(tf.cast(good_test, tf.float32)) / num_ratings_test\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy at step 0: 0.29956782\n",
      "Validation accuracy at step 0: 0.29994312\n",
      "Training cost: 493753.34\n",
      "Validation cost: 3.5464902\n",
      "Training accuracy at step 100: 0.77897906\n",
      "Validation accuracy at step 100: 0.66773725\n",
      "Training cost: 49316.35\n",
      "Validation cost: 0.6316748\n",
      "Training accuracy at step 200: 0.82617563\n",
      "Validation accuracy at step 200: 0.6847\n",
      "Training cost: 39786.23\n",
      "Validation cost: 0.60670036\n"
     ]
    }
   ],
   "source": [
    "#Run training\n",
    "# execute\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "num_epochs = 300\n",
    "for i in range(num_epochs):\n",
    "    if (i%100==0):\n",
    "        res = sess.run([accuracy_tr, accuracy_test, regularized_cost, cost_test])\n",
    "        acc_tr = res[0]\n",
    "        acc_test = res[1]\n",
    "        cost_ev = res[2]\n",
    "        cost_test_ev = res[3]\n",
    "        print(\"Training accuracy at step %s: %s\" % (i, acc_tr))\n",
    "        print(\"Validation accuracy at step %s: %s\" % (i, acc_test))\n",
    "        print(\"Training cost: %s\" % (cost_ev))\n",
    "        print(\"Validation cost: %s\" % (cost_test_ev))\n",
    "    else:\n",
    "        sess.run ([training_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating for user 186 for item 302 is 3 and our prediction is: 3.126236\n",
      "RMSE:0.784305\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "u, p, r = df[['user', 'item', 'rate']].values[1]\n",
    "rhat = tf.gather(tf.gather(tf.add(result, mean_rating), u-1), p-1)\n",
    "print (\"rating for user \" + str(u) + \" for item \" + str(p) + \" is \" + str(r) + \" and our prediction is: \" + str(sess.run(rhat)))\n",
    "\n",
    "# RMSE\n",
    "rmse_cost_test = tf.sqrt(tf.div(tf.reduce_sum(tf.square(diff_op_test, name=\"squared_difference_test\"), name=\"sum_squared_error_test\"), df_test.shape[0] * 2, name=\"average_error\"))\n",
    "\n",
    "print (\"RMSE:\" + str(sess.run(rmse_cost_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
